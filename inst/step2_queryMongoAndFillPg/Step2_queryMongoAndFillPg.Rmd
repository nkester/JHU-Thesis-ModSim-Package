---
title: 'Step 2: Query MongoDB, transform data, and write to PostreSQL DB'
author: "Neil Kester"
date: "4/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      messages = FALSE)
```

# Purpose

These functions execute the extract, transform, and load (ETL) steps taking the data required to support analysis from the simulation's MongoDB logs, structuring and relating it, and then loading it into a relational database (in this case PostgreSQL) in preparation for later analysis.  

# Utility Function

This is a function I wrote in 2019 to convert an R tibble into a `SQLite` or `PostgreSQL` `INSERT` query. It is fairly robust. One limitation is the size of the resulting query string. Deal with this using the `batchSize` parameter to break the queries into manageable chunks.  

```{r batch_fillAndWrite}
#' Utility Batch Fill and Write
#'
#' This is a utility function used in modSim to take a set of data, break it up
#'  into batches and write it to the required PostgreSQL database.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param data A tibble of data
#' @param pgConnParam The connection strings required to connect to the PostgreSQL
#'  database of choice.
#' @param tableName The table of the PostgreSQL table this data is to be inserted
#'  into. It should include `\\"` around names requiring preservation of case.
#'  Optionally: It can include the SQL Field name specification to deal with a
#'  tibble that may not be ordered the same as the SQL Table. This should look
#'  like: `"\"tableName\" (\"fieldOneName\",\"fieldTwoName\")"`
#' @param batchSize How many records to execute at a time.
#' @param database What type of database is it going into? Options are: PostgreSQL
#'  or SQLite.
#'
#' @return Returns nothing
#'
#' @note Location: ./R/fct_utils_batchFillAndWrite.R
batch_fillAndWrite <- function(data,
                               pgConnParam,
                               tableName,
                               batchSize=100,
                               database = "PostgreSQL"){
  
  if(database == "PostgreSQL"){
    
    serial <- "DEFAULT"
    
  }else if(database == "SQLite"){
    
    serial <- "NULL"
    
  }else{
    
    stop("The database parameter must be either \"SQLite\" or \"PostgreSQL\"")
    
  }
  
  
  startSize <- nrow(data)
  
  while(nrow(data) != 0){
    
    if(nrow(data)<batchSize){
      
      query_data <- fillTableQuery(data = data[1:nrow(data),],
                                   tableName = paste0("\"",
                                                      tableName,
                                                      "\" (",
                                                      paste0("\"",
                                                             names(data),
                                                             "\"",
                                                             collapse = ","),
                                                      ")"),
                                   serial = serial)
      
      sendPgFillTableQuery(query = query_data,
                           host = pgConnParam[["pgHost"]],
                           port = pgConnParam[["pgPort"]],
                           user = pgConnParam[["pgUser"]],
                           password = pgConnParam[["pgPass"]],
                           dbname = pgConnParam[["pgDb"]])
      
      data <- data[-(1:nrow(data)),]
      
      rm(query_data)
      
    }else{
      
      query_data <- fillTableQuery(data = data[1:batchSize,],
                                   tableName = paste0("\"",
                                                      tableName,
                                                      "\" (",
                                                      paste0("\"",
                                                             names(data),
                                                             "\"",
                                                             collapse = ","),
                                                      ")"),
                                   serial = serial)
      
      sendPgFillTableQuery(query = query_data,
                           host = pgConnParam[["pgHost"]],
                           port = pgConnParam[["pgPort"]],
                           user = pgConnParam[["pgUser"]],
                           password = pgConnParam[["pgPass"]],
                           dbname = pgConnParam[["pgDb"]])
      
      data <- data[-(1:batchSize),]
      
      rm(query_data)
      
    } # close else
    
    message(paste0((nrow(data)/startSize)*100,"% complete with ",tableName," table!"))
    
  }
  
  
} # batch_fillAndWrite

```


# Low Functions  

## Map Sensors and Entities: `{mapSensorsAndEntities}`  

```{r fct_low_mapSensorsAndEntities}
#' Map Sensors and Entities
#'
#' This function accepts the same inputs as the basic MongoDB connection information
#'  and submits aggregation pipelines to MongoDB to return information about the
#'  sensors and entities present in the specified designPoint. 
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoUri This is a double quoted character string of the connection
#'  object required for the simulation's MongoDB.
#' @param mongoDb This is a double quoted character string of the database name
#'  containing the required files.
#' @param mongoCollection This is a double quoted character string of the mongo
#'  collection within the named database containing the required information.
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#'
#' @return This function returns a four element named list of tibbles that
#'  describe all sensors and entities, their IDs and description.
#'
#' @importFrom tibble tibble
#' @importFrom mongolite mongo
#'
#' @export mapSensorsAndEntities
#'
#' @note Location: ./R/fct_step2_low_mapSensorsAndEntities.R
mapSensorsAndEntities <- function(mongoUri,
                                  mongoDb,
                                  mongoCollection,
                                  desingPoint){

  mongoConnection <- mongolite::mongo(url = mongoUri,
                                      db = mongoDb,
                                      collection = mongoCollection)

  { # SENSOR DESCRIPTION ----
    
    pipeline_sensorDescription <- sprintf("[{\"$match\":{\"%s\": \"WASP1_high_high_20210426\"}},{\"$group\":{\"_id\": {\"sensorId\": \"$state.sensorId\",\"acquireSensorType\": \"$state.acquireSensorType\",\"magnification\": \"$state.magnification\"}}}]",
                                          desingPoint)
    
    sensorDescription <- mongoConnection$aggregate(pipeline = pipeline_sensorDescription)
    names(sensorDescription) <- "id"
    sensorDescription <- tibble::tibble(sensorDescription$id)

  } # close SENSOR DESCRIPTION section

  { # ENTITY ID TO NAME ----
    
    pipeline_entityIdToName <- sprintf("[{\"$match\":{\"%s\": \"WASP1_high_high_20210426\"}},{\"$group\":{\"_id\": {\"entityId\": \"$state.entityId\",\"source\": \"$state.status.source\"}}}]",
                                          desingPoint)
    
    entityIdToName <- mongoConnection$aggregate(pipeline = )
    names(entityIdToName) <- "id"
    entityIdToName <- tibble::tibble(entityIdToName$id)

  } # close ENTITY ID TO NAME section

  { # SENSOR TO ENTITY ----
    
    pipeline_sensorToEntity <- sprintf("[{\"$match\":{\"%s\": \"WASP1_high_high_20210426\"}},{\"$group\":{\"_id\": {\"entityId\": \"$state.entityId\",\"sensorId\": \"$state.sensorId\"}}}]",
                                          desingPoint)
    
    sensorToEntity <- mongoConnection$aggregate(pipeline = pipeline_sensorToEntity)
    names(sensorToEntity) <- "id"
    sensorToEntity <- tibble::tibble(sensorToEntity$id)

  } # close SENSOR TO ENTITY section

  { # META DATA ----
    
    pipeline_metaData <- sprintf("[{\"$match\":{\"%s\": \"WASP1_high_high_20210426\"}},{\"$group\":{\"_id\": {\"runId\": \"$runId\",\"designPoint\": \"$designPoint\",\"iteration\": \"$iteration\"}}}]",
                                          desingPoint)
    
    metaData <- mongoConnection$aggregate(pipeline = pipeline_metaData)
    names(metaData) <- "id"
    metaData <- tibble::tibble(metaData$id)

  } # close META DATA section

  return(list("SensorDescription" = sensorDescription,
              "EntityIdToName" = entityIdToName,
              "SensorToEntityId" = sensorToEntity,
              "metaData" = metaData))

} # close mapSensorsAndEntities function

```


## Line of Sight Tables  

This table describes when a sensor target has Line of Sight (LOS). This does not mean they have acquired each other but rather that their line of sight is not obstructed. 

```{r fct_low_etlLosData}

#' ETL Line of Sight Data
#'
#' This function queries the LOSTargetStatus collection in the simulation
#'  MongoDB, extracts the required fields, and then writes it to the PostgreSQL
#'  tables created by the `creatModSimDb` function. This operates as an iterator
#'  with MongoDB so it executes one record at a time.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoConnParam This is a two element named list including the "mongoUri"
#'   which includes the user name and password and a single character string and
#'   the "mongoDb" name as a character string.
#' @param pgConnParam A five element named list containing the following elements:
#'  "pgHost", "pgPort", "pgUser", "pgPass", and "pgDb".
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#'
#' @return This returns messages to the console updating the user on the function's
#'   status but returns no information.
#'
#' @importFrom dplyr rename mutate
#' @importFrom stringr str_replace_all
#'
#' @note Location: ./R/fct_step2_low_etlLosData.R
etlLosData <- function(mongoConnParam,
                       pgConnParam,
                       designPoint){
  
  requireNamespace(package = "magrittr")
  
  { # Complete the MongoDB Connection Parameters ----
    
    mongoConnParam[["collection"]] <- "AcquireModel.event.LOSTargetStatus"
    
    mongoConnParam[["query"]] <- sprintf("{\"designPoint\": \"%s\"}",
                                         designPoint)
    
    mongoConnParam[["fields"]] <- "{\"_id\": true, \"runId\": true, \"runTime\": true,
  \"designPoint\": true, \"iteration\": true, \"time\": true,\"event\": true}"
    
  } # close Complete the MongoDB Connection Parameters
  
  { # Iterate
    
    message("Beginning Iteration")
    
    #> Connect to the MongoDB
    mongoConn <- mongolite::mongo(url = mongoConnParam$mongoUri,
                                  db = mongoConnParam$mongoDb,
                                  collection = mongoConnParam$collection)
    
    #> Return the number of records present in the query (for status)
    numRecs <- mongoConn$count(query = mongoConnParam$query)
    
    #> Create an iterator (cursor) in the MongoDB
    it <- mongoConn$iterate(query = mongoConnParam$query,
                            fields = mongoConnParam$fields)
    
    #> Connect to the PostgreSQL Database
    pgConn <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
                             host = pgConnParam$pgHost,
                             port = pgConnParam$pgPort,
                             user = pgConnParam$pgUser,
                             password = pgConnParam$pgPass,
                             dbname = pgConnParam$pgDb)
    
    rdx <- 1
    
    #> The iterator returns `null` when it reaches the last record.
    while(!is.null(x <- it$one())){
      
      message(paste0("LOS Row: ",
                     as.character(rdx),
                     "  is ",
                     round(x = rdx/numRecs,
                           digits = 3)*100,
                     "% complete!")
      )
      
      x <- it$one()
      
      temp <- tibble::tibble("losState_pkid" = NA,
                             "id" = x$`_id`,
                             "runId" = x$runId,
                             "runTime" = x$runTime,
                             "designPoint" = x$designPoint,
                             "iteration" = x$iteration,
                             "time_ms" = x$time,
                             "time_s" = x$time/1000,
                             "sensorId" = x$event$sensorId,
                             "targetId" = x$event$targetId,
                             "hasLOS" = x$event$hasLOS) 
      
      temp_query <- fillTableQuery(data = temp,
                                   tableName = "\"losState\"",
                                   serial = "DEFAULT")
      
      DBI::dbSendQuery(conn = pgConn,
                       statement = temp_query)
      
      rdx <- rdx + 1
      
    }
    
    #> Disconnect from the databases when the job is complete.
    DBI::dbDisconnect(conn = pgConn)
    mongoConn$disconnect()
    
  } # close Iterate
  
  message("LOS Complete")
  
} # close fct_low_etlLosData


```

```{r test_fct_low_etlLosData,eval=FALSE}

#> These source files read in the mongoURI and mongoDb elements and the PostgreSQL connection objects.

source("./connectionObjects/pgConnectionObj.R")
source("./connectionObjects/mongoConnectionObj.R")

#> Add to the pgConnParam the database name 

pgConnParam[["pgDb"]] <- "modSim3"

fct_low_etlLosData(mongoConnParam = mongoConnParam,
                   pgConnParam = pgConnParam,
                   designPoint = "NeilsTestScenario")

```
## Sensor Acquisition State Tables  

Unlike the LOS table, this table shows the state of acquisition between each sensor target pair. It also shows the previous acquisition state so we can determine when the acquistion level changes. 

```{r fct_low_etlSensorAcq}
#' ETL Sensor Acquisition Data
#'
#' This function queries the simulation's MongoDB C2SimulationMessage collection,
#'   transforms and unnestes it, and finally writes it to the PostgreSQL tables
#'   created by the `createModSimDb` function.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoConnParamThis is a two element named list including the "mongoUri"
#'   which includes the user name and password and a single character string and
#'   the "mongoDb" name as a character string.
#' @param pgConnParam A five element named list containing the following elements:
#'  "pgHost", "pgPort", "pgUser", "pgPass", and "pgDb".
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#' @param batchSize A numeric integer representing how many records you want to
#'  write to the PostgreSQL database at a time.
#'
#' @return This returns messages to the console updating the user on the function's
#'   status but returns no information.
#'
#' @importFrom dplyr rename mutate
etlSensorAcq <- function(mongoConnParam,pgConnParam,designPoint,batchSize){
  
  requireNamespace(package = "magrittr")
  
  { # Complete the MongoDB Connection Parameters ----
    
    mongoConnParam[["collection"]] <- "AcquireModel.event.C2SimulationMessage"
    
    mongoConnParam[["query"]] <- sprintf("{\"designPoint\": \"%s\", \"event.messageData.javaClass\": \"sensorproto.SensorModel$DetectedTarget\"}",
                                         designPoint)
    
    mongoConnParam[["fields"]] <- "{\"runId\": 1,\"runTime\": 1, \"designPoint\": 1,\"iteration\": 1, \"time\": 1, \"event.receiverId\": 1, \"event.senderId\": 1,\"event.messageData.any.sensorDetection\": 1}"
    
  } # close Complete the MongoDB Connection Parameters
  
  { # Iterate
    
    message("Beginning Iteration")
    
    mongoConn <- mongolite::mongo(url = mongoConnParam$mongoUri,
                                  db = mongoConnParam$mongoDb,
                                  collection = mongoConnParam$collection)
    
    numRecs <- mongoConn$count(query = mongoConnParam$query)
    
    it <- mongoConn$iterate(query = mongoConnParam$query,
                            fields = mongoConnParam$fields)
    
    pgConn <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
                             host = pgConnParam$pgHost,
                             port = pgConnParam$pgPort,
                             user = pgConnParam$pgUser,
                             password = pgConnParam$pgPass,
                             dbname = pgConnParam$pgDb)
    
    rdx <- 1
    
    while(!is.null(x <- it$one())){
    #Testing
    #for(idx in 1:1000){ 
      
      message(paste0("Acq Row: ",as.character(rdx),
                     "  % comp: ",round(x = rdx/numRecs,
                                        digits = 3)*100))
      
      x <- it$one()
      
      temp <- tibble::tibble("sensorAcqState_pkid" = NA,
                             "id" = x$`_id`,
                             "runId" = x$runId,
                             "runTime" = x$runTime,
                             "designPoint" = x$designPoint,
                             "iteration" = x$iteration,
                             "time_ms" = x$time,
                             "time_s" = x$time/1000,
                             "receiverId" = x$event$receiverId,
                             "senderId" = x$event$senderId,
                             "sensorId" = x$event$messageData$any$sensorDetection$sensorId,
                             "entityId" = x$event$messageData$any$sensorDetection$entityId,
                             "targetId" = x$event$messageData$any$sensorDetection$targetId,
                             "detectionLevel" = x$event$messageData$any$sensorDetection$detectionLevel,
                             "previousDetectionLevel" = x$event$messageData$any$sensorDetection$previousDetectionLevel,
                             "timeToDetection" = x$event$messageData$any$sensorDetection$timeToDetection) 
      
      temp_query <- fillTableQuery(data = temp,tableName = "\"sensorAcqState\"",serial = "DEFAULT")
      
      DBI::dbSendQuery(conn = pgConn,
                       statement = temp_query)
      
      rdx <- rdx + 1
      
    }
    
    DBI::dbDisconnect(conn = pgConn)
    mongoConn$disconnect()
    
  } # close Iterate
  
  # { # Extract ----
  #   
  #   message("Extracting data from the MongoDB")
  #   
  #   sensorAcqData <- modSim::sensorAcquisition(mongoUri = mongoConnParam[["mongoUri"]],
  #                                              mongoDb = mongoConnParam[["mongoDb"]],
  #                                              mongoCollection = mongoConnParam[["collection"]],
  #                                              mongoQuery = mongoConnParam[["query"]],
  #                                              mongoFields = mongoConnParam[["fields"]],
  #                                              recursiveUnnests = c("event",
  #                                                                   "messageData",
  #                                                                   "any",
  #                                                                   "sensorDetection"))
  #   
  # } # close Extract
  # 
  # { # Transform and Load ----
  #   
  #   message("Transforming and loading sensorAcqState Data!")
  #   
  #   sensorAcqData <- sensorAcqData %>%
  #     dplyr::rename("id" = "_id",
  #                   "time_ms" = "time") %>%
  #     dplyr::mutate(time_s = time_ms/1000,
  #                   sensorAcqState_pkId = NA)
  #   
  #   batch_fillAndWrite(data = sensorAcqData,
  #                      pgConnParam = pgConnParam,
  #                      tableName = "sensorAcqState",
  #                      batchSize = batchSize,
  #                      database = "PostgreSQL")
  #   
  # } # close Transform and Load
  
  message("Sensor Acq ETL Complete")
  
} # close fct_low_etlSensorAcq

```

```{r test_fct_low_etlSensorAcq, eval=FALSE}

#> These source files read in the mongoURI and mongoDb elements and the PostgreSQL connection objects.

source("../connectionObjects/pgConnectionObj.R")
source("../connectionObjects/mongoConnectionObj.R")

#> Add to the pgConnParam the database name 

pgConnParam[["pgDb"]] <- "modSim3"

fct_low_etlSensorAcq(mongoConnParam = mongoConnParam,
                     pgConnParam = pgConnParam,
                     designPoint = "NeilsTestScenario")
getwd()
```


# Mid Function

## Sensor to Entity Mapping  

Select information from the `AcquireModel.state.sensors` MongoDB collection and write it to the `sensorDescription`, `entityIdToName`, and `sensorToEntityId` PostgreSQL tables.  

These tables provide maps to the various entity and sensor Ids produced by the simulation.  

```{r fct_mid_etlSensorToEntityMappingTables}

#' ETL Sensor To Entity Mapping Tables
#'
#' This function queries the simulation's MongoDB, extracting specific information
#'   from the state.sensors collection, and it writes the resulting data to tables 
#'   in PostgreSQL created by the `createModSimDb` function.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoConnParam This is a two element named list including the "mongoUri"
#'   which includes the user name and password and a single character string and
#'   the "mongoDb" name as a character string.
#' @param pgConnParam A five element named list containing the following elements:
#'  "pgHost", "pgPort", "pgUser", "pgPass", and "pgDb".
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#' @param batchSize A numeric integer representing how many records you want to
#'  write to the PostgreSQL database at a time.
#'
#' @return This returns messages to the console updating the user on the function's
#'   status but returns no information.
#'
#' @importFrom dplyr distinct mutate case_when rename
#' @importFrom stringr str_replace_all str_detect str_extract
#'
#' @note Location: ./R/fct_step2_mid_etlSensorToEntityMappingTables.R
etlSensorToEntityMappingTables <- function(mongoConnParam,
                                           pgConnParam,
                                           designPoint,
                                           batchSize){
  
  requireNamespace(package = "magrittr")
  
  { # Complete the MongoDB Connection Parameters ----
    
    mongoConnParam[["collection"]] <- "AcquireModel.state.sensors"
    
  } # close Complete the MongoDB Connection Parameters
  
  { # Extract ----
    
    { # Query MongoDb and unnest information about sensors and entities ----
      
      message("Extracting data from MongoDB")
      
      entitySensorMapping <- mapSensorsAndEntities(mongoUri = mongoConnParam[["mongoUri"]],
                                                   mongoDb = mongoConnParam[["mongoDb"]],
                                                   mongoCollection = mongoConnParam[["collection"]],
                                                   designPoint)
      
      metaData <- entitySensorMapping$metaData
      
    } # close Query MongoDb and unnest information about sensors and entities section
    
  } # close Extract section
  
  { # Transform and Load ----
    
    { # sensorDescription ----
      
      message("Transforming and loading sensorDescription data.")
      
      entitySensorMapping$SensorDescription <- entitySensorMapping$SensorDescription %>%
        dplyr::mutate(.data = .,
                      designPoint = dplyr::distinct(metaData,
                                                    designPoint)[[1]],
                      sensorId_pkId = NA)
      
      batch_fillAndWrite(data = entitySensorMapping$SensorDescription,
                         pgConnParam = pgConnParam,
                         tableName = "sensorDescription",
                         batchSize = batchSize,
                         database = "PostgreSQL")
      
    } # close sensorDescription section
    
    { # entityIdToName ----
      
      message("Transforming and loading entityIdToName data.")
      
      entitySensorMapping$EntityIdToName <- entitySensorMapping$EntityIdToName %>%
        dplyr::mutate(.data = .,
                      designPoint = dplyr::distinct(metaData,
                                                    designPoint)[[1]],
                      force = dplyr::case_when(
                        stringr::str_detect(string = source,
                                            pattern = "^(?i)(blueforce)") ~ "BLUEFORCE",
                        stringr::str_detect(string = source,
                                            pattern = "^(?i)(redforce)") ~ "REDFORCE",
                        TRUE ~ "OTHER"
                      ),
                      shortName = stringr::str_extract(string = source,
                                                       pattern = "[^/]*$"),
                      entityId_pkId = NA)
      
      batch_fillAndWrite(data = entitySensorMapping$EntityIdToName,
                         pgConnParam = pgConnParam,
                         tableName = "entityIdToName",
                         batchSize = batchSize,
                         database = "PostgreSQL")
      
      
    } # close entityIdToName section
    
    { # sensorToEntityId ----
      
      message("Transforming and loading sensorToEntityId data.")
      
      entitySensorMapping$SensorToEntityId <- entitySensorMapping$SensorToEntityId %>%
        dplyr::mutate(.data = .,
                      designPoint = dplyr::distinct(metaData,
                                                    designPoint)[[1]],
                      sensorToEntityId_pkId = NA)
      
      batch_fillAndWrite(data = entitySensorMapping$SensorToEntityId,
                         pgConnParam = pgConnParam,
                         tableName = "sensorToEntityId",
                         batchSize = batchSize,
                         database = "PostgreSQL")
      
    } # close sensorToEntityId section
    
  } # close Transform and Load section
  
} # close fct_step2_mid_etlSensorToEntityMappingTables


```

```{r test_fct_low_etlSensorToEntityMappingTables,eval = FALSE}

#> These source files read in the mongoURI and mongoDb elements and the PostgreSQL connection objects.

source("./connectionObjects/pgConnectionObj.R")
source("./connectionObjects/mongoConnectionObj.R")

#> Add to the pgConnParam the database name 

pgConnParam[["pgDb"]] <- "modSim3"

fct_low_etlSensorToEntityMappingTables(mongoConnParam = mongoConnParam,
                                       pgConnParam = pgConnParam,
                                       designPoint = "NeilsTestScenario")

```  

# High Function  

This function takes three elements as inputs, `mongoConnParam`, `pgConnParam`, and `designPoint` which are used by each of the low functions. Each low function queries and unpackes different collections from the Simulation MongoDB and stores them in the PostgreSQL Relational Database created in the previous step.  

```{r fct_high_queryMongoAndFillPg}
#' Query MongoDB And Fill PostgreSQL
#'
#' This is the high level function executed in step 2 to extract data from the
#'   simulation's No-SQL MongoDB, transform it, and load it into the analytic
#'   PostgreSQL relational database created with `createModSimDb`. This is
#'   executed for one design point at a time.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoConnParam This is a two element named list including the "mongoUri"
#'   which includes the user name and password and a single character string and
#'   the "mongoDb" name as a character string.
#' @param pgConnParam A five element named list containing the following elements:
#'  "pgHost", "pgPort", "pgUser", "pgPass", and "pgDb".
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#' @param batchSize A numeric integer representing how many records you want to
#'  write to the PostgreSQL database at a time.
#'
#' @return This returns messages to the console updating the user on the function's
#'   status but returns no information.
#'
#' @export queryMongoAndFillPg
#'
queryMongoAndFillPg <- function(mongoConnParam,pgConnParam,designPoint,batchSize=100){
  
  message("Reading from MongDB Acquire.State.Sensor collection and writing to PostgreSQL sensorDescription, entityIdToName, sensorToEntityId, and unnestedSensorState tables.")
  
  etlSensorToEntityMappingTables(mongoConnParam = mongoConnParam,
                                 pgConnParam = pgConnParam,
                                 designPoint = designPoint,
                                 batchSize = batchSize)
  
  message("Reading from MongoDB AcquireModel.event.LOSTargetStatus collection and writing to PostgreSQL losState table.")
  
  etlLosData(mongoConnParam = mongoConnParam,
             pgConnParam = pgConnParam,
             designPoint = designPoint,
             batchSize = batchSize)
  
  message("Reading from MongoDb AcquireModel.event.C2SimulationMessage collection and writing to PostgreSQL sensorAcqState table.")
  
  etlSensorAcq(mongoConnParam = mongoConnParam,
               pgConnParam = pgConnParam,
               designPoint = designPoint,
               batchSize = batchSize)
  
  { # Refresh the materialized Views ----
    
    #> After writing all of the data for this designPoint to the PostgreSQL
    #>  database, update the materialized views so that the data is ready to be
    #>  queried. Failing to do this negates the utility of having a pre-executed
    #>  view ready to be queried.
    
    pgConn <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
                             host = pgConnParam[["pgHost"]],
                             port = pgConnParam[["pgPort"]],
                             user = pgConnParam[["pgUser"]],
                             password = pgConnParam[["pgPass"]],
                             dbname = pgConnParam[["pgDb"]])
    
    DBI::dbSendQuery(conn = pgConn,
                     statement = "REFRESH MATERIALIZED VIEW los_sensor_target_pairs_materialized")
    
    DBI::dbSendQuery(conn = pgConn,
                     statement = "REFRESH MATERIALIZED VIEW acq_sensor_target_pairs_materialized")
    
    DBI::dbDisconnect(conn = pgConn)
    
    rm(pgConn)
    
  } # close Refresh the materialized Views section
  
  return("Complete!")
  
} # close fct_high_queryMongoAndFillPg

```   

This is how this function would be executed:  

```{r, eval=FALSE}

#> These source files read in the mongoURI and mongoDb elements and the PostgreSQL connection objects.

source("../connectionObjects/pgConnectionObj.R")
source("../connectionObjects/mongoConnectionObj.R")

#> Add to the pgConnParam the database name 

pgConnParam[["pgDb"]] <- "modSimIt5"

#> Execute the function  

response <- queryMongoAndFillPg(mongoConnParam = mongoConnParam,
                    pgConnParam = pgConnParam,
                    designPoint = designPoint)


getwd()
```


